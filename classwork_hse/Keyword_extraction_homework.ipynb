{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Здесь перечислены 3 метода улучшения F-меры\n",
    "\n",
    "Сначала предобработаем данные и введем метрики оценки качества. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1000) \n",
    "PATH_TO_DATA = './data'\n",
    "files = [os.path.join(PATH_TO_DATA, file) for file in os.listdir(PATH_TO_DATA) if file.endswith('jsonlines')]\n",
    "data = pd.concat([pd.read_json(file, lines=True) for file in files][:1], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(true_kws, predicted_kws):\n",
    "    assert len(true_kws) == len(predicted_kws)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    jaccards = []\n",
    "    \n",
    "    for i in range(len(true_kws)):\n",
    "        true_kw = set(true_kws[i])\n",
    "        predicted_kw = set(predicted_kws[i])\n",
    "        \n",
    "        tp = len(true_kw & predicted_kw)\n",
    "        union = len(true_kw | predicted_kw)\n",
    "        fp = len(predicted_kw - true_kw)\n",
    "        fn = len(true_kw - predicted_kw)\n",
    "        \n",
    "        if (tp + fp) == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = tp / (tp + fp)\n",
    "        \n",
    "        if (tp + fn) == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = tp / (tp + fn)\n",
    "            \n",
    "        if (prec + rec) == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = (2*(prec*rec))/(prec+rec)\n",
    "            \n",
    "        jac = tp / union\n",
    "        \n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "        f1s.append(f1)\n",
    "        jaccards.append(jac)\n",
    "    \n",
    "    print('Precision - ', round(np.mean(precisions), 2))\n",
    "    print('Recall - ', round(np.mean(recalls), 2))\n",
    "    print('F1 - ', round(np.mean(f1s), 2))\n",
    "    print('Jaccard - ', round(np.mean(jaccards), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  1.0\n",
      "Recall -  1.0\n",
      "F1 -  1.0\n",
      "Jaccard -  1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['keywords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Используем другие стоп-слова из https://pypi.org/project/stop-words/ и выбираем не 10, а 6 самых частотных слов => F-мера улучшается до 0.18\n",
    "\n",
    "Сначала применим только другие стоп-слова. Список этих стоп-слов шире, поэтому F-мера улучшается на 0.01 (=0.17), если мы сравниваем реальные ключевые слова с 10-ю самыми частотными словами. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(get_stop_words('ru'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "    \n",
    "    return words\n",
    "\n",
    "data['content_norm'] = data['content'].apply(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.14\n",
      "Recall -  0.26\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь извлечем не топ-10, а топ-6 самых частотных слов и сравним их с реальными ключевыми словами: F-мера улучшается еще на 0.01 до 0.18. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.18\n",
      "Recall -  0.2\n",
      "F1 -  0.18\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], data['content_norm'].apply(lambda x: [x[0] for x in Counter(x).most_common(6)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Выставляем другие параметры в TfidfVectorizer: \n",
    "min_df=2 (или 3);\n",
    "берется топ-5 слов вместо топ-10\n",
    "===> F-мера улучшается до 0.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.19\n",
      "Recall -  0.18\n",
      "F1 -  0.18\n",
      "Jaccard -  0.11\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "            \n",
    "stops = set(stopwords.words(\"russian\"))\n",
    "            \n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0] for word in words if word and word not in stops]\n",
    "    words = [word.normal_form for word in words if word.tag.POS == 'NOUN']\n",
    "    \n",
    "    return words\n",
    "\n",
    "\n",
    "data['content_norm'] = data['content'].apply(normalize)\n",
    "data['content_norm_str'] = data['content_norm'].apply(' '.join)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=2) #меняем здесь min_df\n",
    "tfidf.fit(data['content_norm_str'])\n",
    "id2word = {i:word for i,word in enumerate(tfidf.get_feature_names())}\n",
    "texts_vectors = tfidf.transform(data['content_norm_str'])\n",
    "keywords = [[id2word[w] for w in top] for top in texts_vectors.toarray().argsort()[:,:-6:-1]] #здесь -11 меняем на -6\n",
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Пробуем TermExtractor ===> F-мера повышается до 0.17\n",
    "https://github.com/igor-shevchenko/rutermextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anyway/anaconda/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "from rutermextract import TermExtractor\n",
    "import math\n",
    "\n",
    "\n",
    "term_extractor = TermExtractor()\n",
    "keywords = []\n",
    "for text in data['content']:\n",
    "    terms = []\n",
    "    for term in term_extractor(text, limit = 5):\n",
    "        terms.append(term.normalized)\n",
    "    keywords.append(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision -  0.18\n",
      "Recall -  0.17\n",
      "F1 -  0.17\n",
      "Jaccard -  0.1\n"
     ]
    }
   ],
   "source": [
    "evaluate(data['keywords'], keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anyway/anaconda/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "#не успела настроить TermExtractor так, чтобы он учитывал еще и idf-веса. \n",
    "\n",
    "\n",
    "def compute_idf(word, corpus):\n",
    "#на вход берется слово, для которого считаем IDF\n",
    "#и корпус документов в виде списка списков слов\n",
    "#количество документов, где встречается искомый термин\n",
    "#считывается как генератор списков\n",
    "    return math.log10(len(corpus)/sum([1.0 for i in corpus if word in i]))\n",
    "\n",
    "term_extractor = TermExtractor()\n",
    "keywords_all = []\n",
    "for text in data['content']:\n",
    "    terms = []\n",
    "    for term in term_extractor(text, limit = 5):\n",
    "        terms.append(term.normalized)\n",
    "    keywords.append(terms)\n",
    "        \n",
    "texts = keywords\n",
    "\n",
    "idf = {}\n",
    "for text in keywords_all:\n",
    "    for word in text:\n",
    "        if word not in idf:\n",
    "            idf[word] = compute_idf(word, key)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "print(idf)\n",
    "\n",
    "'''keywords_ranked = []\n",
    "for text in data['content_norm']:\n",
    "    ms = term_extractor(''.join(text), weight=lambda term: idf.get(term), limit = 10)\n",
    "    keywords_ranked.append(terms)\n",
    "\n",
    "print(keywords_ranked) '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
